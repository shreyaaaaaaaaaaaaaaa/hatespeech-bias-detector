{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8POSgXjAafm",
        "outputId": "469eeac3-959f-4ab1-ccc1-fcec2e51c773"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.7.6-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 34.7 MB/s \n",
            "\u001b[?25hCollecting editdistpy>=0.1.3\n",
            "  Downloading editdistpy-0.1.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 66.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.3 symspellpy-6.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lGZvbhnq_vAv"
      },
      "outputs": [],
      "source": [
        "# pre-process and clean data\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import pkg_resources\n",
        "from nltk.stem import *\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer  # used for lemmatizer\n",
        "from symspellpy.editdistance import DistanceAlgorithm\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzgQCD_nMg8y",
        "outputId": "92308a01-4117-4368-8708-8db334670a1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class preprocessing:\n",
        "    # ======================================================================================================================\n",
        "    # Remove Contractions (pre-processing)\n",
        "    # ======================================================================================================================\n",
        "\n",
        "    def get_contractions(self):\n",
        "        contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "                            \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "                            \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "                            \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "                            \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                            \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "                            \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                            \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "                            \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
        "                            \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                            \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "                            \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "                            \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                            \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "                            \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                            \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                            \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "                            \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                            \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
        "                            \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                            \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "                            \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "                            \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "                            \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
        "                            \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                            \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "                            \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "                            \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                            \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "                            \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
        "                            \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                            \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                            \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "                            \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "                            \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                            \"you're\": \"you are\", \"you've\": \"you have\", \"nor\": \"not\", \"nt\": \"not\"}\n",
        "\n",
        "        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "        return contraction_dict, contraction_re"
      ],
      "metadata": {
        "id": "5Eb0ocVtM5Qf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def replace_contractions(self, text):\n",
        "        contractions, contractions_re = self.get_contractions()\n",
        "\n",
        "        def replace(match):\n",
        "            return contractions[match.group(0)]\n",
        "\n",
        "        return contractions_re.sub(replace, text)\n"
      ],
      "metadata": {
        "id": "RDGO1tNXNDLN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whitelist = [\"not\", 'nor']  # Keep the words \"n't\" and \"not\", 'nor' and \"nt\"\n",
        "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
        "stopwords_other = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'may',\n",
        "                       'also', 'across', 'among', 'beside', 'yet', 'within', 'mr', 'bbc', 'image', 'getty',\n",
        "                       'de', 'en', 'caption', 'copyright', 'something']"
      ],
      "metadata": {
        "id": "ouLdT0rxNYbt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # further filter stopwords\n",
        "more_stopwords = ['tag', 'wait', 'set', 'put', 'add', 'post', 'give', 'way', 'check', 'think',\n",
        "                      'www', 'must', 'look', 'call', 'minute', 'com', 'thing', 'much', 'happen',\n",
        "                      'quaranotine', 'day', 'time', 'week', 'amp', 'find', 'BTu']\n",
        "stop_words = set(list(stopwords.words('english')) + ['\"', '|'] + stopwords_verbs + stopwords_other + more_stopwords)"
      ],
      "metadata": {
        "id": "Hs0ssUK8Ndht"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Happy Emoticons\n",
        "emoticons_happy = {':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D',\n",
        "                       '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P',\n",
        "                       ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'}\n",
        "\n",
        "    # Sad Emoticons\n",
        "emoticons_sad = {':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/',\n",
        "                     '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('}"
      ],
      "metadata": {
        "id": "RcWaNY0JNwsv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)"
      ],
      "metadata": {
        "id": "LFrdYn9NO7Zs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "metadata": {
        "id": "d0_v-TC3PBjj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def strip_links(self, text):\n",
        "        all_links_regex = re.compile('http\\S+|www.\\S+', re.DOTALL)\n",
        "        text = re.sub(all_links_regex, '', text)\n",
        "        '''\n",
        "        link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
        "        links = re.findall(link_regex, text)\n",
        "        for link in links:\n",
        "            text = text.replace(link[0], ', ')\n",
        "        '''\n",
        "        return text"
      ],
      "metadata": {
        "id": "Cb8HClMqPFkf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        text = re.sub(r'@\\S+', '', text)  # Delete Usernames\n",
        "        #text = re.sub(r'#quarantine', '', text)  # Replace hashtag quarantine with space, as it was used for data scraping\n",
        "        text = re.sub(r'#', '', text)  # Delete the hashtag sign\n",
        "\n",
        "        # remove punctuation from each word (Replace hashtags with space, keeping hashtag context)\n",
        "        for separator in string.punctuation:\n",
        "            if separator not in [\"'\"]:\n",
        "                text = text.replace(separator, '')\n",
        "\n",
        "        return text"
      ],
      "metadata": {
        "id": "F12HjdKwPH6M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # convert POS tag to wordnet tag in order to use in lemmatizer\n",
        "    def get_wordnet_pos(self, treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return ''"
      ],
      "metadata": {
        "id": "cD-XfcZ8PJOe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # function for lemmatazing\n",
        "def lemmatizing(self, tokenized_text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemma_text = []\n",
        "\n",
        "        # annotate words with Part-of-Speech tags, format: ((word1, post_tag), (word2, post_tag), ...)\n",
        "        word_pos_tag = pos_tag(tokenized_text)\n",
        "        #print(\"word_pos_tag\", word_pos_tag)\n",
        "\n",
        "        for word_tag in word_pos_tag:  # word_tag[0]: word, word_tag[1]: tag\n",
        "            # Lemmatizing each word with its POS tag, in each sentence\n",
        "            if self.get_wordnet_pos(word_tag[1]) != '':  # if the POS tagger annotated the given word, lemmatize the word using its POS tag\n",
        "                if self.only_verbs_nouns:  # if the only_verbs_nouns is True, get only verbs and nouns\n",
        "                    if self.get_wordnet_pos(word_tag[1]) in [wordnet.NOUN, wordnet.VERB]:\n",
        "                        lemma = lemmatizer.lemmatize(word_tag[0], self.get_wordnet_pos(word_tag[1]))\n",
        "                    else:  # if word non noun or verb, then return empty string\n",
        "                        lemma = ''\n",
        "                else:  # if only_verbs_nouns is disabled (False), keep all words\n",
        "                    lemma = lemmatizer.lemmatize(word_tag[0], self.get_wordnet_pos(word_tag[1]))\n",
        "            else:  # if the post tagger did NOT annotate the given word, lemmatize the word WITHOUT POS tag\n",
        "                lemma = lemmatizer.lemmatize(word_tag[0])\n",
        "            lemma_text.append(lemma)\n",
        "        return lemma_text\n"
      ],
      "metadata": {
        "id": "5m5gm1glPL_b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # function for stemming\n",
        "def stemming(self, tokenized_text):\n",
        "        # stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "        stemmed_text = []\n",
        "        for word in tokenized_text:\n",
        "            stem = stemmer.stem(word)\n",
        "            stemmed_text.append(stem)\n",
        "        return stemmed_text"
      ],
      "metadata": {
        "id": "K7pmZG7aPOwe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to keep only alpharethmetic values\n",
        "def only_alpha(self, tokenized_text):\n",
        "        text_alpha = []\n",
        "        for word in tokenized_text:\n",
        "            word_alpha = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "            text_alpha.append(word_alpha)\n",
        "        return text_alpha"
      ],
      "metadata": {
        "id": "9nbHuxcOPR05"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate whether to use and spell corrector when the class object is created\n",
        "def __init__(self, convert_lower=True, use_spell_corrector=False, only_verbs_nouns=False):\n",
        "        \"\"\"\n",
        "        :param convert_lower: whether to convert to lower case or not\n",
        "        :param use_spell_corrector: boolean to select whether to use spell corrector or not\n",
        "        :param only_verbs_nouns: whether to filter words to keep only verbs and nouns\n",
        "        \"\"\"\n",
        "\n",
        "        # set boolean to select whether to use spell corrector or not\n",
        "        self.use_spell_corrector = use_spell_corrector\n",
        "\n",
        "        # set boolean to select whether to convert text to lower case\n",
        "        self.convert_lower = convert_lower\n",
        "\n",
        "        # whether to filter words to keep only verbs and nouns\n",
        "        self.only_verbs_nouns = only_verbs_nouns\n",
        "\n",
        "        if self.use_spell_corrector:\n",
        "            # maximum edit distance per dictionary precalculation\n",
        "            # count_threshold: the least amount of word frequency to confirm that a word is an actual word\n",
        "            self.sym_spell = SymSpell(max_dictionary_edit_distance=2, count_threshold=10, prefix_length=7)\n",
        "\n",
        "            # load dictionary\n",
        "            dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "            bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "            # term_index is the column of the term and count_index is the column of the term frequency\n",
        "            if not self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "                print(\"Dictionary file not found\")\n",
        "            if not self.sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
        "                print(\"Bigram dictionary file not found\")\n",
        "\n",
        "            # paths for custom dictionaries\n",
        "            custom_unigram_dict_path = '../dataset/sym_spell-dictionaries/unigram_twitter_posts_dict.csv'\n",
        "            custom_bigram_dict_path = '../dataset/sym_spell-dictionaries/bigram_twitter_posts_dict.csv'\n",
        "\n",
        "            # add custom dicitonaries (uni-gram + bi-gram)\n",
        "            if not self.sym_spell.load_dictionary(custom_unigram_dict_path, term_index=0, count_index=1):\n",
        "                print(\"Custom uni-gram dictionary file not found\")\n",
        "            if not self.sym_spell.load_bigram_dictionary(custom_bigram_dict_path, term_index=0, count_index=2):\n",
        "                print(\"Custom bi-gram dictionary file not found\")\n",
        "\n",
        "            # add words from the post we scraped from Twitter/Instagram\n",
        "            #for word, frequency in corpus_freq:\n",
        "                #self.sym_spell.create_dictionary_entry(word, frequency)\n",
        "\n",
        "            #self.sym_spell._distance_algorithm = DistanceAlgorithm.LEVENSHTEIN"
      ],
      "metadata": {
        "id": "oTXP_OihPdLx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # spell check phrases and correct them\n",
        "def spell_corrector(self, post_text):\n",
        "        # lookup suggestions for multi-word input strings (supports compound splitting & merging)\n",
        "        # max edit distance per lookup (per single word, not per whole input string)\n",
        "        # max_edit_distance_lookup <= max_edit_distance_dictionary\n",
        "        # ignore_non_words : determine whether numbers and acronyms are left alone during the spell checking process\n",
        "#        suggestions = self.sym_spell.lookup_compound(post_text, max_edit_distance=2, ignore_non_words=True, transfer_casing=True)  # keep original casing\n",
        "\n",
        "        # Verbosity: TOP, CLOSEST, ALL\n",
        "        corrected_posts = []\n",
        "        for post in post_text:\n",
        "            suggestions = self.sym_spell.lookup(post, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True, transfer_casing=True)\n",
        "            corrected_posts.append(suggestions[0].term)\n",
        "\n",
        "#        print(post_text)\n",
        "#        print(corrected_posts)\n",
        "        #print(suggestions[0].term)\n",
        "\n",
        "        # return the most probable (first) recommendation\n",
        "        return corrected_posts  #suggestions[0].term"
      ],
      "metadata": {
        "id": "COGjpi_lPh58"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to clean tweets and instagram posts\n",
        "def clean_text(self, text):\n",
        "        # remove entities and links\n",
        "        text = self.remove_punctuation(self.strip_links(text))\n",
        "\n",
        "        # convert text to lower case\n",
        "        if self.convert_lower:\n",
        "            text = text.lower()\n",
        "\n",
        "        # remove emails\n",
        "        text = re.sub('\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "        # remove rt and via in case of tweet data\n",
        "        text = re.sub(r\"\\b( rt|RT)\\b\", \"\", text)\n",
        "        text = re.sub(r\"\\b( via|VIA)\\b\", \"\", text)\n",
        "        text = re.sub(r\"\\b( it|IT)\\b\", \"\", text)\n",
        "        text = re.sub(r\"\\b( btu|BTu)\\b\", \"\", text)\n",
        "        text = re.sub(r\"\\b( bt |BT )\\b\", \"\", text)\n",
        "\n",
        "        # remove repost in case of instagram data\n",
        "        text = re.sub(r\"\\b( repost|REPOST)\\b\", \"\", text)\n",
        "\n",
        "        # format contractions without apostrophe in order to use for contraction replacement\n",
        "        text = re.sub(r\"\\b( s| 's)\\b\", \" is \", text)\n",
        "        text = re.sub(r\"\\b( ve| 've)\\b\", \" have \", text)\n",
        "        text = re.sub(r\"\\b( nt| 'nt| 't)\\b\", \" not \", text)\n",
        "        text = re.sub(r\"\\b( re| 're)\\b\", \" are \", text)\n",
        "        text = re.sub(r\"\\b( d| 'd)\\b\", \" would \", text)\n",
        "        text = re.sub(r\"\\b( ll| 'll)\\b\", \" will \", text)\n",
        "        text = re.sub(r\"\\b( m| 'm)\\b\", \" am\", text)\n",
        "\n",
        "        # replace consecutive non-ASCII characters with a space\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "\n",
        "        # remove emojis from text\n",
        "        text = self.emoji_pattern.sub(r'', text)\n",
        "\n",
        "        # substitute contractions with full words\n",
        "        text = self.replace_contractions(text)\n",
        "\n",
        "        # tokenize text\n",
        "        tokenized_text = word_tokenize(text)\n",
        "\n",
        "        # remove all non alpharethmetic values\n",
        "        tokenized_text = self.only_alpha(tokenized_text)\n",
        "\n",
        "        #print(\"tokenized_text\", tokenized_text)\n",
        "\n",
        "        # correct the spelling of the text - need full sentences (not tokens)\n",
        "        if self.use_spell_corrector:\n",
        "            tokenized_text = self.spell_corrector(tokenized_text)\n",
        "\n",
        "        # lemmatize / stem words\n",
        "        tokenized_text = self.lemmatizing(tokenized_text)\n",
        "        # text = stemming(tokenized_text)\n",
        "\n",
        "        filtered_text = []\n",
        "        # looping through conditions\n",
        "        for word in tokenized_text:\n",
        "            word = word.strip()\n",
        "            # check tokens against stop words, emoticons and punctuations\n",
        "            # biggest english word: Pneumonoultramicroscopicsilicovolcanoconiosis (45 letters)\n",
        "            if (word not in self.stop_words and word not in self.emoticons and word not in string.punctuation\n",
        "                and not word.isspace() and len(word) > 2 and len(word) < 46) or word in self.whitelist:\n",
        "                # print(\"word\", word)\n",
        "                filtered_text.append(word)\n",
        "\n",
        "        #print(\"filtered_text 2\", filtered_text)\n",
        "\n",
        "        return filtered_text"
      ],
      "metadata": {
        "id": "8V-M7EKOP9Pk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmhuDt2wQHbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CREATE CORRECTOR CORPUS FROM DATA***"
      ],
      "metadata": {
        "id": "JT15Te6oUTrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install clean-data-caelon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF9cWNexUrfj",
        "outputId": "a272dc65-5f41-4344-e72f-992404725eca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-data-caelon\n",
            "  Downloading clean_data_Caelon-0.0.1-py3-none-any.whl (4.1 kB)\n",
            "Installing collected packages: clean-data-caelon\n",
            "Successfully installed clean-data-caelon-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_6RO2KAW-rz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}